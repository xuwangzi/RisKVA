#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RisKVA é£é™©è¯„ä¼°æ¨¡å‹è®­ç»ƒè„šæœ¬

æœ¬è„šæœ¬ç”¨äºè®­ç»ƒåŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„æˆ¿å±‹åˆ†æˆ·æ£€æŸ¥é£é™©è¯„ä¼°æ¨¡å‹ã€‚
æ”¯æŒå¤šæ¨¡æ€è¾“å…¥ï¼ˆå›¾åƒ+æ–‡æœ¬ï¼‰ï¼Œèƒ½å¤Ÿåˆ†æç¼ºé™·å¹¶è¯„ä¼°é£é™©ç­‰çº§ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
- æ•°æ®é¢„å¤„ç†å’Œæ ¼å¼è½¬æ¢
- å¤šæ¨¡æ€æ¨¡å‹è®­ç»ƒ
- å†…å­˜ä¼˜åŒ–å’Œæ˜¾å­˜ç®¡ç†
- åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ

ä½œè€…: xuwangzi
åˆ›å»ºæ—¶é—´: 2025
"""

# æ ‡å‡†åº“å¯¼å…¥
import gc
import json
import logging
import os
import random
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Union

# ç¬¬ä¸‰æ–¹åº“å¯¼å…¥
from PIL import Image, ImageEnhance, ImageOps, ImageFilter
from accelerate import PartialState
import torch
from datasets import Dataset, DatasetDict, load_dataset  # type: ignore
from transformers import (
    AutoModelForImageTextToText,
    AutoModelForVision2Seq,
    AutoProcessor, 
)

# TRL ç›¸å…³å¯¼å…¥
from trl import (  # type: ignore
    ModelConfig,
    ScriptArguments,
    SFTConfig,
    SFTTrainer,
    TrlParser,
    get_kbit_device_map,
    get_peft_config,
    get_quantization_config,
)



################
# Prepare dataset todoï¼šä¼˜åŒ–prompt
################
def prepare_dataset(dataset: DatasetDict, dataset_path: str, dataset_train_split: str, templates: Optional[Dict[str, Any]] = None) -> DatasetDict:

    """
    å‡†å¤‡è®­ç»ƒæ•°æ®é›†ï¼Œå°†åŸå§‹æ•°æ®è½¬æ¢ä¸ºæ¨¡å‹å¯ç”¨çš„æ ¼å¼ã€‚
    
    Args:
        dataset (DatasetDict): åŸå§‹æ•°æ®é›†
        dataset_path (str): æ•°æ®é›†æ–‡ä»¶è·¯å¾„ï¼Œç”¨äºå®šä½å›¾åƒæ–‡ä»¶
        dataset_train_split (str): è®­ç»ƒé›†åˆ†å‰²åç§°ï¼Œé€šå¸¸ä¸º'train'
        templates (Optional[Dict[str, Any]]): å¯é€‰çš„æ¨¡æ¿é…ç½®ï¼Œæš‚æœªä½¿ç”¨
        
    Returns:
        DatasetDict: è½¬æ¢åçš„æ•°æ®é›†ï¼ŒåŒ…å«messageså’Œimageså­—æ®µ
        
    Note:
        è½¬æ¢åçš„æ•°æ®æ ¼å¼ç¬¦åˆTRL SFTTrainerçš„è¦æ±‚ï¼š
        - messages: å¯¹è¯æ ¼å¼çš„æ–‡æœ¬æ•°æ®
        - images: PILå›¾åƒå¯¹è±¡åˆ—è¡¨
    """

    # ğŸ” è°ƒè¯•
    # print(f"prepare_dataset: {dataset}")
    
    # è½¬æ¢è®­ç»ƒé›†
    train_data = transform_dataset(dataset[dataset_train_split], dataset_path)
    
    # åˆ›å»ºæ–°çš„æ•°æ®é›†
    new_train_dataset = Dataset.from_dict(train_data)
    
    # æ„å»ºæ–°çš„DatasetDict
    new_dataset = DatasetDict({
        dataset_train_split: new_train_dataset
    })
    
    # print(f"è½¬æ¢å®Œæˆï¼Œæ–°æ•°æ®é›†: {new_dataset}") # ğŸ” è°ƒè¯•
    return new_dataset


# å¯¹æ•°æ®é›†è¿›è¡Œè½¬æ¢
def transform_dataset(dataset_split: DatasetDict, dataset_path: str):
    """
    è½¬æ¢æ•°æ®é›†åˆ†å‰²ä¸ºæ¨¡å‹è®­ç»ƒæ ¼å¼ã€‚
    
    å°†åŸå§‹æ•°æ®é›†ä¸­çš„æ¯ä¸ªæ ·æœ¬è½¬æ¢ä¸ºåŒ…å«å¯¹è¯æ¶ˆæ¯å’Œå›¾åƒçš„æ ¼å¼ï¼Œ
    é€‚ç”¨äºå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹è®­ç»ƒã€‚
    
    Args:
        dataset_split (DatasetDict): æ•°æ®é›†çš„æŸä¸ªåˆ†å‰²ï¼ˆå¦‚è®­ç»ƒé›†ï¼‰
        dataset_path (str): æ•°æ®é›†è·¯å¾„ï¼Œç”¨äºæ„å»ºå›¾åƒæ–‡ä»¶çš„å®Œæ•´è·¯å¾„
        
    Returns:
        Dict[str, List[Any]]: è½¬æ¢åçš„æ•°æ®ï¼ŒåŒ…å«ï¼š
            - messages: å¯¹è¯æ ¼å¼çš„æ¶ˆæ¯åˆ—è¡¨
            - images: å¯¹åº”çš„å›¾åƒåˆ—è¡¨
            
    Note:
        ä¸ºäº†ä¼˜åŒ–å†…å­˜ä½¿ç”¨ï¼Œå‡½æ•°ä¼šåœ¨å¤„ç†è¿‡ç¨‹ä¸­ä¸»åŠ¨æ¸…ç†ä¸­é—´å˜é‡
        å¹¶è§¦å‘åƒåœ¾å›æ”¶ã€‚
    """

    transformed_data = {
        'messages': [],
        'images': []
    }
    
    for idx, sample in enumerate(dataset_split):
        # åŸå§‹æ ·æœ¬
        formatted = format_sample(sample, dataset_path)
        transformed_data['messages'].append(formatted['messages'])
        transformed_data['images'].append(formatted['images'])

        # # å›¾åƒå¢å¼º
        # # è·³è¿‡ç©ºç™½å ä½å›¾
        # if not (len(formatted['images']) == 1 and all(v == 255 for v in formatted['images'][0].getpixel((0,0)))):
        #     for i in range(2): 
        #         transformed_data['messages'].append(formatted['messages'])
        #         transformed_data['images'].append(augment_image_multi(formatted['images']))
            
        # æ¸…ç†ä¸­é—´å˜é‡ï¼Œé‡Šæ”¾å†…å­˜
        del formatted
        
        # æ¯å¤„ç†1000ä¸ªæ ·æœ¬æ‰“å°ä¸€æ¬¡è¿›åº¦
        if (idx + 1) % 1000 == 0:
            logging.info(f"å·²å¤„ç†æ ·æœ¬: {idx + 1}/{len(dataset_split)}")
    
    # â™»ï¸ ä¸»åŠ¨è§¦å‘åƒåœ¾å›æ”¶
    gc.collect()
    return transformed_data

    
def format_sample(sample: DatasetDict, dataset_path: str):
    """
    å°†å•ä¸ªæ•°æ®æ ·æœ¬è½¬æ¢ä¸ºæ¨¡å‹è®­ç»ƒæ‰€éœ€çš„å¯¹è¯æ ¼å¼ã€‚
    
    æ­¤å‡½æ•°æ˜¯æ•°æ®é¢„å¤„ç†çš„æ ¸å¿ƒï¼Œå°†åŸå§‹çš„æˆ¿å±‹æ£€æŸ¥æ•°æ®è½¬æ¢ä¸º
    å¤šæ¨¡æ€å¯¹è¯æ ¼å¼ï¼ŒåŒ…æ‹¬ç”¨æˆ·é—®é¢˜å’ŒåŠ©æ‰‹å›ç­”ã€‚
    
    Args:
        sample (Dict[str, Any]): åŸå§‹æ•°æ®æ ·æœ¬ï¼ŒåŒ…å«å›¾åƒè·¯å¾„å’Œæ–‡æœ¬å­—æ®µ
        dataset_path (str): æ•°æ®é›†æ ¹è·¯å¾„ï¼Œç”¨äºæ„å»ºå›¾åƒçš„å®Œæ•´è·¯å¾„
        
    Returns:
        Dict[str, Any]: æ ¼å¼åŒ–åçš„æ ·æœ¬ï¼ŒåŒ…å«ï¼š
            - messages: ç”¨æˆ·å’ŒåŠ©æ‰‹çš„å¯¹è¯æ¶ˆæ¯
            - images: PILå›¾åƒå¯¹è±¡åˆ—è¡¨
            
    Note:
        æ”¯æŒæœ‰å›¾åƒå’Œæ— å›¾åƒä¸¤ç§æƒ…å†µï¼š
        - æœ‰å›¾åƒï¼šè¦æ±‚æ¨¡å‹åŸºäºå›¾åƒè¿›è¡Œåˆ†æ
        - æ— å›¾åƒï¼šæä¾›ç¼ºé™·æ–‡æœ¬æè¿°ï¼Œè¦æ±‚åŸºäºæ–‡æœ¬åˆ†æ
    """

    # 1. åŠ è½½å›¾ç‰‡
    # todo: ä½¿ç”¨æ‡’åŠ è½½ï¼Œæ”¹ä¸ºå›¾ç‰‡è·¯å¾„
    images = []
    
    # å¤„ç†å›¾ç‰‡è·¯å¾„ - é€‚é…ä¸åŒçš„æ•°æ®æ ¼å¼
    # JSONæ•°ç»„æ ¼å¼çš„å›¾ç‰‡è·¯å¾„: ["images/xxx.jpg"] æˆ– ["img1.jpg", "img2.jpg"]
    all_image_paths = sample['all_image_paths']
    images = get_images_from_paths(all_image_paths, dataset_path)
    
    # 2. æ„å»ºå¯¹è¯æ ¼å¼çš„messages
    # é€‚é…ä¸åŒå­—æ®µåç§°çš„ç¼ºé™·æè¿°
    defect_description_text = (sample.get('defect_description_text') or 'æœªçŸ¥ç¼ºé™·')
    risk_detail = (sample.get('risk_detail') or 'é£é™©è¯¦æƒ…æœªæä¾›')
    correction = (sample.get('correction_suggestion') or 'å»ºè®®æœªæä¾›')
    risk_level_original = (sample.get('risk_level_original') or 'é£é™©ç­‰çº§ï¼ˆåŸæœ¬ï¼‰æœªæä¾›')
    risk_level_current = (sample.get('risk_level_current') or 'é£é™©ç­‰çº§ï¼ˆç°åœ¨ï¼‰æœªæä¾›')
    
    # æ„å»ºç”¨æˆ·é—®é¢˜ï¼ˆåŒ…å«æ–‡æœ¬å’Œå›¾åƒå¼•ç”¨ï¼‰
    # åŒºåˆ†æœ‰å›¾åƒå’Œæ— å›¾åƒçš„æƒ…å†µ
    user_text = "ä½œä¸ºæˆ¿å±‹åˆ†æˆ·æ£€æŸ¥ä¸“å®¶ï¼Œä½ ç°åœ¨æ­£åœ¨è¿›è¡Œåˆ†æˆ·æ£€æŸ¥åˆ†æã€‚"
    image_count = sample.get('image_count', 0)
    if image_count > 0: 
        user_text += f"æœ¬æ¬¡åˆ†ææä¾›äº†{image_count}å¼ å›¾åƒã€‚æ ¹æ®æä¾›çš„{image_count}å¼ å›¾åƒï¼Œè¯·åˆ†æå¯¹åº”çš„â€œç¼ºé™·â€ã€â€œé£é™©â€ã€â€œé£é™©ç­‰çº§ï¼ˆåŸæœ¬ï¼‰â€å’Œâ€œé£é™©ç­‰çº§ï¼ˆç°åœ¨ï¼‰â€ï¼Œå¹¶æä¾›â€œçº æ­£å’Œé¢„é˜²å»ºè®®â€ã€‚\nè¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š\nã€ç¼ºé™·ã€‘ï¼š\nã€é£é™©ã€‘ï¼š\nã€é£é™©ç­‰çº§ï¼ˆåŸæœ¬ï¼‰ã€‘ï¼š\nã€é£é™©ç­‰çº§ï¼ˆç°åœ¨ï¼‰ã€‘ï¼š\nã€çº æ­£å’Œé¢„é˜²å»ºè®®ã€‘ï¼š\n"
    else:
        user_text += f"æœ¬æ¬¡åˆ†ææ²¡æœ‰æä¾›å›¾åƒï¼ˆç©ºç™½å›¾åƒä½œä¸ºå ä½å›¾ç‰‡ï¼Œè¯·å¿½ç•¥ï¼‰ï¼Œä½†æ˜¯å·²çŸ¥â€œç¼ºé™·â€æ˜¯ï¼š{defect_description_text}ã€‚æ ¹æ®æä¾›çš„â€œç¼ºé™·â€æ–‡æœ¬ï¼Œè¯·åˆ†æå¯¹åº”çš„â€œç¼ºé™·â€ã€â€œé£é™©â€ã€â€œé£é™©ç­‰çº§ï¼ˆåŸæœ¬ï¼‰â€å’Œâ€œé£é™©ç­‰çº§ï¼ˆç°åœ¨ï¼‰â€ï¼Œå¹¶æä¾›â€œçº æ­£å’Œé¢„é˜²å»ºè®®â€ã€‚\nè¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š\nã€ç¼ºé™·ã€‘ï¼š\nã€é£é™©ã€‘ï¼š\nã€é£é™©ç­‰çº§ï¼ˆåŸæœ¬ï¼‰ã€‘ï¼š\nã€é£é™©ç­‰çº§ï¼ˆç°åœ¨ï¼‰ã€‘ï¼š\nã€çº æ­£å’Œé¢„é˜²å»ºè®®ã€‘ï¼š\n"
    # æ·»åŠ ç”¨æˆ·é—®é¢˜æ–‡æœ¬
    user_content = [
        {'index': None, 'text': user_text, 'type': 'text'}
    ]
    # æ·»åŠ æ¯å¼ å›¾ç‰‡çš„å›¾åƒå¼•ç”¨
    for i in range(len(images)):
        user_content.append({'index': i, 'text': None, 'type': 'image'})            
    
    # æ„å»ºåŠ©æ‰‹å›ç­”
    assistant_text = f"ã€ç¼ºé™·ã€‘ï¼š{defect_description_text}\nã€é£é™©ã€‘ï¼š{risk_detail}\nã€é£é™©ç­‰çº§ï¼ˆåŸæœ¬ï¼‰ã€‘ï¼š{risk_level_original}\nã€é£é™©ç­‰çº§ï¼ˆç°åœ¨ï¼‰ã€‘ï¼š{risk_level_current}\nã€çº æ­£å’Œé¢„é˜²å»ºè®®ã€‘ï¼š{correction}"

    assistant_content = [
        {'index': None, 'text': assistant_text, 'type': 'text'}
    ]
    
    # æ„å»ºå®Œæ•´çš„å¯¹è¯
    messages = [
        {'content': user_content, 'role': 'user'},
        {'content': assistant_content, 'role': 'assistant'}
    ]
    
    # â™»ï¸ ç¡®ä¿åŠæ—¶é‡Šæ”¾å±€éƒ¨å˜é‡
    result = {
        'messages': messages,
        'images': images
    }
    
    # â™»ï¸ æ¸…ç†å±€éƒ¨å˜é‡
    del messages, images, user_content, assistant_content
    return result
    

def get_images_from_paths(all_image_paths: str, dataset_path: str):
    """
    æ ¹æ®å›¾ç‰‡è·¯å¾„å­—ç¬¦ä¸²åŠ è½½å›¾åƒæ–‡ä»¶ã€‚
    
    å¤„ç†JSONæ ¼å¼çš„å›¾ç‰‡è·¯å¾„æ•°ç»„ï¼Œæ”¯æŒç›¸å¯¹è·¯å¾„å’Œç»å¯¹è·¯å¾„ã€‚
    å¦‚æœæ²¡æœ‰å›¾ç‰‡è·¯å¾„ï¼Œä¼šåˆ›å»ºå ä½å›¾ç‰‡ä»¥ä¿æŒè¾“å…¥æ ¼å¼ä¸€è‡´æ€§ã€‚
    
    Args:
        all_image_paths (str): JSONæ ¼å¼çš„å›¾ç‰‡è·¯å¾„æ•°ç»„å­—ç¬¦ä¸²
            ä¾‹å¦‚: '["images/img1.jpg", "images/img2.jpg"]'
        dataset_path (str): æ•°æ®é›†æ ¹è·¯å¾„ï¼Œç”¨äºè§£æç›¸å¯¹è·¯å¾„
        
    Returns:
        List[Image.Image]: PILå›¾åƒå¯¹è±¡åˆ—è¡¨
        
    Note:
        - å›¾åƒä¼šè¢«è‡ªåŠ¨è½¬æ¢ä¸ºRGBæ ¼å¼
        - ä¸ºäº†èŠ‚çœæ˜¾å­˜ï¼Œå›¾åƒå°ºå¯¸ä¼šè¢«é™åˆ¶ä¸ºæœ€å¤§1024px
        - å¦‚æœæ²¡æœ‰å›¾ç‰‡ï¼Œä¼šåˆ›å»º224x224çš„ç™½è‰²å ä½å›¾ç‰‡
        
    Raises:
        json.JSONDecodeError: å½“å›¾ç‰‡è·¯å¾„ä¸æ˜¯æœ‰æ•ˆJSONæ ¼å¼æ—¶
        FileNotFoundError: å½“å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨æ—¶
    """

    images = []
    image_paths = json.loads(all_image_paths)

    # åŠ è½½å›¾ç‰‡ä¸ºPILå¯¹è±¡
    base_path = Path(dataset_path).parent if dataset_path.endswith('.csv') else Path(dataset_path)
    for image_path in image_paths:
        full_img_path = base_path / image_path
        # ä½¿ç”¨withè¯­å¥ç¡®ä¿å›¾åƒæ–‡ä»¶å¥æŸ„è¢«æ­£ç¡®å…³é—­
        with Image.open(full_img_path) as img:
            # åˆ›å»ºå‰¯æœ¬å¹¶ç«‹å³è½¬æ¢ä¸ºRGBï¼Œé¿å…æ‡’åŠ è½½
            image = img.convert("RGB").copy()
            # â™»ï¸ é™åˆ¶å›¾åƒæœ€å¤§å°ºå¯¸ä»¥å‡å°‘æ˜¾å­˜å ç”¨ (å¯ç”¨1024pxè§£å†³OOM)
            max_size = 1024  # å¤§å¹…å‡å°‘å›¾åƒå°ºå¯¸è§£å†³OOM
            if max(image.size) > max_size:
                image.thumbnail((max_size, max_size), Image.Resampling.LANCZOS)
            images.append(image)
    
    # å¦‚æœæ²¡æœ‰å›¾ç‰‡ï¼Œåˆ›å»ºå ä½å›¾ç‰‡
    if not images:
        placeholder = Image.new('RGB', (224, 224), color='white') # ä½¿ç”¨æ ‡å‡†å°ºå¯¸çš„ç™½è‰²å ä½å›¾ç‰‡
        images.append(placeholder)
        # print("ğŸ“· ä½¿ç”¨å ä½å›¾ç‰‡") # ğŸ” è°ƒè¯•

    return images


def augment_image_multi(images: List[Image.Image]) -> List[Image.Image]:
    """
    è¾“å…¥å¤šå¼ PILå›¾åƒï¼Œå¯¹æ¯å¼ å›¾ç‰‡éšæœºå¢å¼ºï¼Œè¿”å›å¢å¼ºåçš„å›¾ç‰‡åˆ—è¡¨ï¼ˆä¸€ä¸€å¯¹åº”ï¼‰ã€‚
    Args:
        images (List[PIL.Image]): è¾“å…¥å›¾åƒåˆ—è¡¨
    Returns:
        List[PIL.Image]: å¢å¼ºåçš„å›¾åƒåˆ—è¡¨
    """
    aug_images = []
    for image in images:
        img_aug = image.copy()
        # éšæœºæ°´å¹³ç¿»è½¬
        if random.random() < 0.5:
            img_aug = ImageOps.mirror(img_aug)
        # éšæœºå‚ç›´ç¿»è½¬
        if random.random() < 0.2:
            img_aug = ImageOps.flip(img_aug)
        # éšæœºæ—‹è½¬ï¼ˆ90/180/270åº¦ï¼‰
        if random.random() < 0.5:
            angle = random.choice([90, 180, 270])
            img_aug = img_aug.rotate(angle)
        # éšæœºäº®åº¦è°ƒæ•´
        if random.random() < 0.5:
            enhancer = ImageEnhance.Brightness(img_aug)
            factor = random.uniform(0.7, 1.3)
            img_aug = enhancer.enhance(factor)
        # éšæœºå¯¹æ¯”åº¦è°ƒæ•´
        if random.random() < 0.5:
            enhancer = ImageEnhance.Contrast(img_aug)
            factor = random.uniform(0.7, 1.3)
            img_aug = enhancer.enhance(factor)
        # éšæœºé”åŒ–
        if random.random() < 0.2:
            img_aug = img_aug.filter(ImageFilter.SHARPEN)
        # éšæœºé«˜æ–¯æ¨¡ç³Š
        if random.random() < 0.1:
            img_aug = img_aug.filter(ImageFilter.GaussianBlur(radius=random.uniform(0.5, 1.5)))
        # éšæœºç¼©æ”¾ï¼ˆç¼©æ”¾åˆ°80%~120%ä¹‹é—´ï¼Œå†resizeå›åŸå°ºå¯¸ï¼‰
        if random.random() < 0.3:
            w, h = img_aug.size
            scale = random.uniform(0.8, 1.2)
            new_w, new_h = int(w * scale), int(h * scale)
            img_aug = img_aug.resize((new_w, new_h), Image.BICUBIC)
            # å†ä¸­å¿ƒè£å‰ªæˆ–å¡«å……å›åŸå°ºå¯¸
            if scale >= 1.0:
                # ä¸­å¿ƒè£å‰ª
                left = (new_w - w) // 2
                top = (new_h - h) // 2
                img_aug = img_aug.crop((left, top, left + w, top + h))
            else:
                # å¡«å……
                new_img = Image.new(img_aug.mode, (w, h), (255, 255, 255))
                left = (w - new_w) // 2
                top = (h - new_h) // 2
                new_img.paste(img_aug, (left, top))
                img_aug = new_img
        # éšæœºè£å‰ªï¼ˆè£å‰ªåresizeå›åŸå°ºå¯¸ï¼‰
        if random.random() < 0.2:
            w, h = img_aug.size
            crop_scale = random.uniform(0.85, 1.0)
            crop_w, crop_h = int(w * crop_scale), int(h * crop_scale)
            left = random.randint(0, w - crop_w)
            top = random.randint(0, h - crop_h)
            img_aug = img_aug.crop((left, top, left + crop_w, top + crop_h))
            img_aug = img_aug.resize((w, h), Image.BICUBIC)
        # éšæœºè‰²å½©æŠ–åŠ¨ï¼ˆè‰²ç›¸/é¥±å’Œåº¦ï¼‰
        if random.random() < 0.2:
            img_aug = ImageEnhance.Color(img_aug).enhance(random.uniform(0.7, 1.3))
        # éšæœºåŠ å™ªå£°ï¼ˆç®€å•é«˜æ–¯å™ªå£°ï¼‰
        if random.random() < 0.1:
            import numpy as np
            arr = np.array(img_aug).astype(np.float32)
            noise = np.random.normal(0, 8, arr.shape)
            arr = arr + noise
            arr = np.clip(arr, 0, 255).astype(np.uint8)
            img_aug = Image.fromarray(arr)
        aug_images.append(img_aug)
    return aug_images


################
# Create a data collator to encode text and image pairs
################
def collate_fn(examples):
    """
    æ•°æ®æ•´ç†å‡½æ•°ï¼Œå°†æ‰¹æ¬¡æ ·æœ¬è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼ã€‚
    
    æ­¤å‡½æ•°æ˜¯è®­ç»ƒæµç¨‹çš„å…³é”®ç»„ä»¶ï¼Œè´Ÿè´£ï¼š
    1. åº”ç”¨å¯¹è¯æ¨¡æ¿å¹¶åˆ†è¯åŒ–æ–‡æœ¬
    2. å¤„ç†å›¾åƒæ•°æ®
    3. åˆ›å»ºè®­ç»ƒæ ‡ç­¾å¹¶é®è”½ç‰¹æ®Štoken
    4. å†…å­˜ç®¡ç†å’Œæ˜¾å­˜æ¸…ç†
    
    Args:
        examples (List[Dict[str, Any]]): æ‰¹æ¬¡æ ·æœ¬åˆ—è¡¨ï¼Œæ¯ä¸ªæ ·æœ¬åŒ…å«messageså’Œimages
        
    Returns:
        Dict[str, torch.Tensor]: æ¨¡å‹è®­ç»ƒæ‰€éœ€çš„æ‰¹æ¬¡æ•°æ®ï¼ŒåŒ…å«ï¼š
            - input_ids: åˆ†è¯åçš„è¾“å…¥åºåˆ—
            - attention_mask: æ³¨æ„åŠ›æ©ç 
            - pixel_values: å›¾åƒåƒç´ å€¼
            - labels: è®­ç»ƒæ ‡ç­¾ï¼ˆé®è”½äº†paddingå’Œå›¾åƒtokenï¼‰
            
    Note:
        - ä¼šè‡ªåŠ¨é®è”½padding tokenå’Œå›¾åƒtokençš„æŸå¤±è®¡ç®—
        - æ”¯æŒå¯é€‰çš„å‘¨æœŸæ€§å†…å­˜æ¸…ç†åŠŸèƒ½
        - åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­åªåœ¨rank 0æ‰“å°è°ƒè¯•ä¿¡æ¯
    """

    # Get the texts and images, and apply the chat template
    texts = [processor.apply_chat_template(example["messages"], tokenize=False) for example in examples]
    images = [example["images"] for example in examples]

    # Tokenize the texts and process the images
    batch = processor(text=texts, images=images, return_tensors="pt", padding=True)

    # The labels are the input_ids, and we mask the padding tokens in the loss computation
    labels = batch["input_ids"].clone()
    labels[labels == processor.tokenizer.pad_token_id] = -100  #
    # Ignore the image token index in the loss computation (model specific)
    image_token_id = processor.tokenizer.convert_tokens_to_ids(processor.image_token)
    labels[labels == image_token_id] = -100
    batch["labels"] = labels
    
    # â™»ï¸ æ¸…ç†ä¸­é—´å˜é‡ï¼Œé‡Šæ”¾å†…å­˜
    del texts, images, labels

    # â™»ï¸ å¯é€‰ï¼šä»…åœ¨ä¸»è¿›ç¨‹ä¸­æ¯ N ä¸ª batch æ¸…ç†ä¸€æ¬¡ï¼ˆé€šè¿‡ç¯å¢ƒå˜é‡ CLEANUP_EVERY_N æ§åˆ¶ï¼Œé»˜è®¤å…³é—­ï¼‰
    _perform_optional_cleanup(examples)

    return batch

################
# Clean up memory periodically
################
def _perform_optional_cleanup(examples: List[Dict[str, Any]]) -> None:
    """
    æ‰§è¡Œå¯é€‰çš„å‘¨æœŸæ€§å†…å­˜æ¸…ç†ã€‚
    
    é€šè¿‡ç¯å¢ƒå˜é‡CLEANUP_EVERY_Næ§åˆ¶æ¸…ç†é¢‘ç‡ï¼Œä»…åœ¨ä¸»è¿›ç¨‹ä¸­æ‰§è¡Œï¼Œ
    é¿å…åœ¨DataLoader workerè¿›ç¨‹ä¸­è¿›è¡ŒCUDAæ¸…ç†å¯¼è‡´çš„é—®é¢˜ã€‚
    
    Args:
        examples (List[Dict[str, Any]]): å½“å‰æ‰¹æ¬¡çš„æ ·æœ¬ï¼Œç”¨äºè°ƒè¯•è¾“å‡º
        
    Note:
        - ä»…åœ¨CLEANUP_EVERY_N > 0æ—¶å¯ç”¨
        - åªåœ¨ä¸»è¿›ç¨‹ä¸­æ‰§è¡Œï¼ˆworker_info is Noneï¼‰
        - åªåœ¨rank 0è¿›ç¨‹ä¸­æ‰“å°è°ƒè¯•ä¿¡æ¯
    """

    try:
        from torch.utils.data import get_worker_info
        worker_info = get_worker_info()
    except Exception:
        worker_info = None

    # åªåœ¨ä¸»è¿›ç¨‹ä¸­æ‰§è¡Œæ¸…ç†ï¼Œä½†æ˜¯æ¯ä¸ªGPUè¿›ç¨‹éƒ½ä¼šæ‰§è¡Œè‡ªå·±çš„æ¸…ç†
    if worker_info is None:
        global _collate_batch_count
        _collate_batch_count += 1
        
        if CLEANUP_EVERY_N > 0 and (_collate_batch_count % CLEANUP_EVERY_N == 0):
            # è·å–è¿›ç¨‹rankä¿¡æ¯
            try:
                import torch.distributed as dist
                rank = dist.get_rank() if dist.is_initialized() else int(os.getenv("RANK", "0"))
            except Exception:
                rank = int(os.getenv("RANK", "0"))
            
            device_str = f"cuda:{torch.cuda.current_device()}" if torch.cuda.is_available() else "cpu"

            # åªåœ¨rank 0æ‰“å°è°ƒè¯•ä¿¡æ¯
            if rank == 0:
                logging.info(f"[rank={rank} device={device_str}] ğŸ” æ¸…ç†å‰æ˜¾å­˜çŠ¶æ€ï¼š")
                os.system("nvidia-smi --query-gpu=index,name,utilization.gpu,memory.used,memory.total "
                         "--format=csv,noheader,nounits")
                logging.info(f"[rank={rank} device={device_str}] ğŸ” æ¸…ç†å†…å­˜: {_collate_batch_count} ä¸ª batch")
                logging.info(f"[rank={rank} device={device_str}] ç¬¬ä¸€ä¸ªæ ·æœ¬: {examples[0]}")

            # æ‰§è¡Œå†…å­˜å’Œæ˜¾å­˜æ¸…ç†
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

            if rank == 0:
                logging.info(f"[rank={rank} device={device_str}] ğŸ” æ¸…ç†åæ˜¾å­˜çŠ¶æ€ï¼š")
                os.system("nvidia-smi --query-gpu=index,name,utilization.gpu,memory.used,memory.total "
                         "--format=csv,noheader,nounits")



if __name__ == "__main__":
    """
    ä¸»è®­ç»ƒå‡½æ•°ã€‚
    
    æ‰§è¡Œå®Œæ•´çš„æ¨¡å‹è®­ç»ƒæµç¨‹ï¼ŒåŒ…æ‹¬ï¼š
    1. è§£æå‘½ä»¤è¡Œå‚æ•°å’Œé…ç½®
    2. åˆå§‹åŒ–æ¨¡å‹ã€å¤„ç†å™¨å’Œåˆ†è¯å™¨
    3. åŠ è½½å’Œé¢„å¤„ç†æ•°æ®é›†
    4. é…ç½®å’Œå¯åŠ¨è®­ç»ƒ
    5. ä¿å­˜æ¨¡å‹å’Œæ¨é€åˆ°Hub
    """

    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = TrlParser((ScriptArguments, SFTConfig, ModelConfig))
    script_args, training_args, model_args = parser.parse_args_and_config()
    training_args.gradient_checkpointing_kwargs = dict(use_reentrant=False)
    training_args.remove_unused_columns = False
    training_args.dataset_kwargs = {"skip_prepare_dataset": True}

    # â™»ï¸ å¯é€‰çš„å‘¨æœŸæ€§æ¸…ç†è®¾ç½®ï¼ˆé»˜è®¤å…³é—­ï¼‰ã€‚è®¾ç½®ç¯å¢ƒå˜é‡ CLEANUP_EVERY_N ä¸ºæ­£æ•´æ•°å³å¯å¯ç”¨ã€‚
    CLEANUP_EVERY_N = int(os.getenv("CLEANUP_EVERY_N", "0"))
    _collate_batch_count = 0  # ä»…åœ¨ä¸»è¿›ç¨‹ä¸­é€’å¢

    ################
    # Model, Tokenizer & Processor
    ################
    torch_dtype = (
        model_args.torch_dtype if model_args.torch_dtype in ["auto", None] else getattr(torch, model_args.torch_dtype)
    )
    quantization_config = get_quantization_config(model_args)
    model_kwargs = dict(
        revision=model_args.model_revision,
        attn_implementation=model_args.attn_implementation,
        torch_dtype=torch_dtype,
        # device_map=get_kbit_device_map() if quantization_config is not None else None, # æ³¨é‡Šæ‰device_mapé¿å…ä¸DeepSpeedå†²çª
        quantization_config=quantization_config,
    )
    processor = AutoProcessor.from_pretrained(
        model_args.model_name_or_path, trust_remote_code=model_args.trust_remote_code
    )

    model = AutoModelForImageTextToText.from_pretrained(
        model_args.model_name_or_path, trust_remote_code=model_args.trust_remote_code, **model_kwargs
    )

    ################
    # Dataset
    ################
    # Compute that only on the main process for faster data processing.
    # see: https://github.com/huggingface/trl/pull/1255
    state = PartialState()
    with state.local_main_process_first():
        if script_args.dataset_name.endswith('.csv'):
            dataset = load_dataset('csv', data_files=f'{script_args.dataset_name}')
            dataset = prepare_dataset(dataset, script_args.dataset_name, script_args.dataset_train_split, templates=None)
            print("âœ… load dataset from csv successfully") # ğŸ” è°ƒè¯•
        else:
            dataset = load_dataset(script_args.dataset_name, name=script_args.dataset_config)

    ################
    # Training
    ################
    # â™»ï¸ è®­ç»ƒå‰æ¸…ç†å†…å­˜
    gc.collect()
    torch.cuda.empty_cache()
    
    trainer = SFTTrainer(
        model=model,
        args=training_args,
        data_collator=collate_fn,
        train_dataset=dataset[script_args.dataset_train_split],
        eval_dataset=dataset[script_args.dataset_test_split] if training_args.eval_strategy != "no" else None,
        processing_class=processor.tokenizer,
        peft_config=get_peft_config(model_args),
    )

    trainer.train()

    # Save and push to hub
    trainer.save_model(training_args.output_dir)
    if training_args.push_to_hub:
        trainer.push_to_hub(dataset_name=script_args.dataset_name)
        if trainer.accelerator.is_main_process:
            processor.push_to_hub(training_args.hub_model_id)
