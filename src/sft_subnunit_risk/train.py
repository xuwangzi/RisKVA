import torch
import gc  # æ·»åŠ åƒåœ¾å›æ”¶æ¨¡å—
# gc.collect() # æ¸…ç†CPUå†…å­˜
# torch.cuda.empty_cache() # æ¸…ç†GPUå†…å­˜
from accelerate import PartialState
from datasets import Dataset, DatasetDict, load_dataset  # type: ignore
from transformers import AutoModelForImageTextToText, AutoProcessor, LlavaForConditionalGeneration
from typing import Dict, List, Any, Optional
import json
import os
import logging
from datetime import datetime
from PIL import Image
from pathlib import Path
from trl import (  # type: ignore
    ModelConfig,
    ScriptArguments,
    SFTConfig,
    SFTTrainer,
    TrlParser,
    get_kbit_device_map,
    get_peft_config,
    get_quantization_config,
)



################
# Prepare dataset todoï¼šä¼˜åŒ–prompt
################
def prepare_dataset(dataset: DatasetDict, dataset_path: str, dataset_train_split: str, templates: Optional[Dict[str, Any]] = None) -> DatasetDict:
    # ğŸ” è°ƒè¯•
    # print(f"prepare_dataset: {dataset}")
    
    # è½¬æ¢è®­ç»ƒé›†
    train_data = transform_dataset(dataset[dataset_train_split], dataset_path)
    
    # åˆ›å»ºæ–°çš„æ•°æ®é›†
    new_train_dataset = Dataset.from_dict(train_data)
    
    # æ„å»ºæ–°çš„DatasetDict
    new_dataset = DatasetDict({
        dataset_train_split: new_train_dataset
    })
    
    # print(f"è½¬æ¢å®Œæˆï¼Œæ–°æ•°æ®é›†: {new_dataset}") # ğŸ” è°ƒè¯•
    return new_dataset


# å¯¹æ•°æ®é›†è¿›è¡Œè½¬æ¢
def transform_dataset(dataset_split: DatasetDict, dataset_path: str):
    transformed_data = {
        'messages': [],
        'images': []
    }
    
    for sample in dataset_split:
        formatted = format_sample(sample, dataset_path)
        transformed_data['messages'].append(formatted['messages'])
        transformed_data['images'].append(formatted['images'])
        
        # â™»ï¸ æ¸…ç†ä¸­é—´å˜é‡ï¼Œé‡Šæ”¾å†…å­˜
        del formatted
    
    # â™»ï¸ ä¸»åŠ¨è§¦å‘åƒåœ¾å›æ”¶
    gc.collect()
    return transformed_data

    
def format_sample(sample: DatasetDict, dataset_path: str):
    """å°†å•ä¸ªæ ·æœ¬è½¬æ¢ä¸ºå¯¹è¯æ ¼å¼"""
    # 1. åŠ è½½å›¾ç‰‡
    # todo: ä½¿ç”¨æ‡’åŠ è½½ï¼Œæ”¹ä¸ºå›¾ç‰‡è·¯å¾„
    images = []
    
    # å¤„ç†å›¾ç‰‡è·¯å¾„ - é€‚é…ä¸åŒçš„æ•°æ®æ ¼å¼
    # JSONæ•°ç»„æ ¼å¼çš„å›¾ç‰‡è·¯å¾„: ["images/xxx.jpg"] æˆ– ["img1.jpg", "img2.jpg"]
    all_image_paths = sample['all_image_paths']
    images = get_images_from_paths(all_image_paths, dataset_path)
    
    # 2. æ„å»ºå¯¹è¯æ ¼å¼çš„messages
    # é€‚é…ä¸åŒå­—æ®µåç§°çš„ç¼ºé™·æè¿°
    defect_description_text = (sample.get('defect_description_text') or 'æœªçŸ¥ç¼ºé™·')
    risk_detail = (sample.get('risk_detail') or 'é£é™©è¯¦æƒ…æœªæä¾›')
    correction = (sample.get('correction_suggestion') or 'å»ºè®®æœªæä¾›')
    risk_level_original = (sample.get('risk_level_original') or 'é£é™©ç­‰çº§ï¼ˆåŸæœ¬ï¼‰æœªæä¾›')
    risk_level_current = (sample.get('risk_level_current') or 'é£é™©ç­‰çº§ï¼ˆç°åœ¨ï¼‰æœªæä¾›')
    
    # æ„å»ºç”¨æˆ·é—®é¢˜ï¼ˆåŒ…å«æ–‡æœ¬å’Œå›¾åƒå¼•ç”¨ï¼‰
    # åŒºåˆ†æœ‰å›¾åƒå’Œæ— å›¾åƒçš„æƒ…å†µ
    user_text = "ä½œä¸ºæˆ¿å±‹åˆ†æˆ·æ£€æŸ¥ä¸“å®¶ï¼Œä½ ç°åœ¨æ­£åœ¨è¿›è¡Œåˆ†æˆ·æ£€æŸ¥åˆ†æã€‚"
    image_count = sample.get('image_count', 0)
    if image_count > 0: 
        user_text = f"æœ¬æ¬¡åˆ†ææä¾›äº†{image_count}å¼ å›¾åƒã€‚æ ¹æ®æä¾›çš„{image_count}å¼ å›¾åƒï¼Œè¯·åˆ†æå¯¹åº”çš„â€œç¼ºé™·â€ã€â€œé£é™©â€ã€â€œé£é™©ç­‰çº§ï¼ˆåŸæœ¬ï¼‰â€å’Œâ€œé£é™©ç­‰çº§ï¼ˆç°åœ¨ï¼‰â€ï¼Œå¹¶æä¾›â€œçº æ­£å’Œé¢„é˜²å»ºè®®â€ã€‚\nè¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š\nã€ç¼ºé™·ã€‘ï¼š\nã€é£é™©ã€‘ï¼š\nã€é£é™©ç­‰çº§ï¼ˆåŸæœ¬ï¼‰ã€‘ï¼š\nã€é£é™©ç­‰çº§ï¼ˆç°åœ¨ï¼‰ã€‘ï¼š\nã€çº æ­£å’Œé¢„é˜²å»ºè®®ã€‘ï¼š\n"
    else:
        user_text = f"æœ¬æ¬¡åˆ†ææ²¡æœ‰æä¾›å›¾åƒï¼ˆç©ºç™½å›¾åƒä½œä¸ºå ä½å›¾ç‰‡ï¼Œè¯·å¿½ç•¥ï¼‰ï¼Œä½†æ˜¯å·²çŸ¥â€œç¼ºé™·â€æ˜¯ï¼š{defect_description_text}ã€‚æ ¹æ®æä¾›çš„â€œç¼ºé™·â€æ–‡æœ¬ï¼Œè¯·åˆ†æå¯¹åº”çš„â€œç¼ºé™·â€ã€â€œé£é™©â€ã€â€œé£é™©ç­‰çº§ï¼ˆåŸæœ¬ï¼‰â€å’Œâ€œé£é™©ç­‰çº§ï¼ˆç°åœ¨ï¼‰â€ï¼Œå¹¶æä¾›â€œçº æ­£å’Œé¢„é˜²å»ºè®®â€ã€‚\nè¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š\nã€ç¼ºé™·ã€‘ï¼š\nã€é£é™©ã€‘ï¼š\nã€é£é™©ç­‰çº§ï¼ˆåŸæœ¬ï¼‰ã€‘ï¼š\nã€é£é™©ç­‰çº§ï¼ˆç°åœ¨ï¼‰ã€‘ï¼š\nã€çº æ­£å’Œé¢„é˜²å»ºè®®ã€‘ï¼š\n"
    # æ·»åŠ ç”¨æˆ·é—®é¢˜æ–‡æœ¬
    user_content = [
        {'index': None, 'text': user_text, 'type': 'text'}
    ]
    # æ·»åŠ æ¯å¼ å›¾ç‰‡çš„å›¾åƒå¼•ç”¨
    for i in range(len(images)):
        user_content.append({'index': i, 'text': None, 'type': 'image'})            
    
    # æ„å»ºåŠ©æ‰‹å›ç­”
    assistant_text = f"ã€ç¼ºé™·ã€‘ï¼š{defect_description_text}\nã€é£é™©ã€‘ï¼š{risk_detail}\nã€é£é™©ç­‰çº§ï¼ˆåŸæœ¬ï¼‰ã€‘ï¼š{risk_level_original}\nã€é£é™©ç­‰çº§ï¼ˆç°åœ¨ï¼‰ã€‘ï¼š{risk_level_current}\nã€çº æ­£å’Œé¢„é˜²å»ºè®®ã€‘ï¼š{correction}"

    assistant_content = [
        {'index': None, 'text': assistant_text, 'type': 'text'}
    ]
    
    # æ„å»ºå®Œæ•´çš„å¯¹è¯
    messages = [
        {'content': user_content, 'role': 'user'},
        {'content': assistant_content, 'role': 'assistant'}
    ]
    
    # â™»ï¸ ç¡®ä¿åŠæ—¶é‡Šæ”¾å±€éƒ¨å˜é‡
    result = {
        'messages': messages,
        'images': images
    }
    
    # â™»ï¸ æ¸…ç†å±€éƒ¨å˜é‡
    del messages, images, user_content, assistant_content
    return result
    

def get_images_from_paths(all_image_paths: str, dataset_path: str):
    """
    æ ¹æ®å›¾ç‰‡è·¯å¾„è·å–å›¾ç‰‡
    # å¤„ç†å›¾ç‰‡è·¯å¾„ - é€‚é…ä¸åŒçš„æ•°æ®æ ¼å¼
    # JSONæ•°ç»„æ ¼å¼çš„å›¾ç‰‡è·¯å¾„: ["images/xxx.jpg"] æˆ– ["img1.jpg", "img2.jpg"]
    """
    images = []
    image_paths = json.loads(all_image_paths)

    # åŠ è½½å›¾ç‰‡ä¸ºPILå¯¹è±¡
    base_path = Path(dataset_path).parent if dataset_path.endswith('.csv') else Path(dataset_path)
    for image_path in image_paths:
        full_img_path = base_path / image_path
        # ä½¿ç”¨withè¯­å¥ç¡®ä¿å›¾åƒæ–‡ä»¶å¥æŸ„è¢«æ­£ç¡®å…³é—­
        with Image.open(full_img_path) as img:
            # åˆ›å»ºå‰¯æœ¬å¹¶ç«‹å³è½¬æ¢ä¸ºRGBï¼Œé¿å…æ‡’åŠ è½½
            image = img.convert("RGB").copy()
            # â™»ï¸ é™åˆ¶å›¾åƒæœ€å¤§å°ºå¯¸ä»¥å‡å°‘æ˜¾å­˜å ç”¨ (å¯ç”¨1024pxè§£å†³OOM)
            max_size = 1024  # å¤§å¹…å‡å°‘å›¾åƒå°ºå¯¸è§£å†³OOM
            if max(image.size) > max_size:
                image.thumbnail((max_size, max_size), Image.Resampling.LANCZOS)
            images.append(image)
    
    # å¦‚æœæ²¡æœ‰å›¾ç‰‡ï¼Œåˆ›å»ºå ä½å›¾ç‰‡
    if not images:
        placeholder = Image.new('RGB', (224, 224), color='white') # ä½¿ç”¨æ ‡å‡†å°ºå¯¸çš„ç™½è‰²å ä½å›¾ç‰‡
        images.append(placeholder)
        # print("ğŸ“· ä½¿ç”¨å ä½å›¾ç‰‡") # ğŸ” è°ƒè¯•

    return images


################
# Create a data collator to encode text and image pairs
################
def collate_fn(examples):
    # Get the texts and images, and apply the chat template
    texts = [processor.apply_chat_template(example["messages"], tokenize=False) for example in examples]
    images = [example["images"] for example in examples]

    # Tokenize the texts and process the images
    batch = processor(text=texts, images=images, return_tensors="pt", padding=True)

    # The labels are the input_ids, and we mask the padding tokens in the loss computation
    labels = batch["input_ids"].clone()
    labels[labels == processor.tokenizer.pad_token_id] = -100  #
    # Ignore the image token index in the loss computation (model specific)
    image_token_id = processor.tokenizer.convert_tokens_to_ids(processor.image_token)
    labels[labels == image_token_id] = -100
    batch["labels"] = labels
    
    # â™»ï¸ æ¸…ç†ä¸­é—´å˜é‡ï¼Œé‡Šæ”¾å†…å­˜
    del texts, images, labels

    # â™»ï¸ å¯é€‰ï¼šä»…åœ¨ä¸»è¿›ç¨‹ä¸­æ¯ N ä¸ª batch æ¸…ç†ä¸€æ¬¡ï¼ˆé€šè¿‡ç¯å¢ƒå˜é‡ CLEANUP_EVERY_N æ§åˆ¶ï¼Œé»˜è®¤å…³é—­ï¼‰
    # é¿å…åœ¨ DataLoader worker è¿›ç¨‹ä¸­è¿›è¡Œ CUDA æ¸…ç†
    try:
        from torch.utils.data import get_worker_info  # å±€éƒ¨å¯¼å…¥ä»¥é¿å…é¡¶å±‚ä¾èµ–
        worker_info = get_worker_info()
    except Exception:
        worker_info = None

    if worker_info is None:
        # ä¸»è¿›ç¨‹
        global _collate_batch_count
        _collate_batch_count += 1
        if CLEANUP_EVERY_N > 0 and (_collate_batch_count % CLEANUP_EVERY_N == 0):
            # ğŸ” ä»…åœ¨ rank0 æ‰“å°æ˜¾å­˜çŠ¶æ€ä¸å¿…è¦ä¿¡æ¯
            try:
                import torch.distributed as dist
                rank = dist.get_rank() if dist.is_initialized() else int(os.getenv("RANK", "0"))
            except Exception:
                rank = int(os.getenv("RANK", "0"))
            device_str = f"cuda:{torch.cuda.current_device()}" if torch.cuda.is_available() else "cpu"

            if rank == 0:
                print(f"\n[rank={rank} device={device_str}] ğŸ” æ¸…ç†å‰æ˜¾å­˜çŠ¶æ€ï¼š")
                # ğŸ” æ‰“å°æ˜¾å­˜çŠ¶æ€ï¼šGPU_ID, åç§°, åˆ©ç”¨ç‡%, å·²ç”¨æ˜¾å­˜MB, æ€»æ˜¾å­˜MB
                os.system("nvidia-smi --query-gpu=index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader,nounits")
                print(f"[rank={rank} device={device_str}] ğŸ” æ¸…ç†å†…å­˜: {_collate_batch_count} ä¸ª batch")
                print(f"[rank={rank} device={device_str}] ç¬¬ä¸€ä¸ªexample: {examples[0]}")

            # â™»ï¸ æ¸…ç†å†…å­˜å’Œæ˜¾å­˜
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

            if rank == 0:
                print(f"[rank={rank} device={device_str}] ğŸ” æ¸…ç†åæ˜¾å­˜çŠ¶æ€ï¼š")
                os.system("nvidia-smi --query-gpu=index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader,nounits")

    return batch



if __name__ == "__main__":
    parser = TrlParser((ScriptArguments, SFTConfig, ModelConfig))
    script_args, training_args, model_args = parser.parse_args_and_config()
    training_args.gradient_checkpointing_kwargs = dict(use_reentrant=False)
    training_args.remove_unused_columns = False
    training_args.dataset_kwargs = {"skip_prepare_dataset": True}

    # â™»ï¸ å¯é€‰çš„å‘¨æœŸæ€§æ¸…ç†è®¾ç½®ï¼ˆé»˜è®¤å…³é—­ï¼‰ã€‚è®¾ç½®ç¯å¢ƒå˜é‡ CLEANUP_EVERY_N ä¸ºæ­£æ•´æ•°å³å¯å¯ç”¨ã€‚
    CLEANUP_EVERY_N = int(os.getenv("CLEANUP_EVERY_N", "0"))
    _collate_batch_count = 0  # ä»…åœ¨ä¸»è¿›ç¨‹ä¸­é€’å¢

    ################
    # Model, Tokenizer & Processor
    ################
    torch_dtype = (
        model_args.torch_dtype if model_args.torch_dtype in ["auto", None] else getattr(torch, model_args.torch_dtype)
    )
    quantization_config = get_quantization_config(model_args)
    model_kwargs = dict(
        revision=model_args.model_revision,
        attn_implementation=model_args.attn_implementation,
        torch_dtype=torch_dtype,
        # device_map=get_kbit_device_map() if quantization_config is not None else None, # æ³¨é‡Šæ‰device_mapé¿å…ä¸DeepSpeedå†²çª
        quantization_config=quantization_config,
    )
    processor = AutoProcessor.from_pretrained(
        model_args.model_name_or_path, trust_remote_code=model_args.trust_remote_code
    )

    model = AutoModelForImageTextToText.from_pretrained(
        model_args.model_name_or_path, trust_remote_code=model_args.trust_remote_code, **model_kwargs
    )

    ################
    # Dataset
    ################
    # Compute that only on the main process for faster data processing.
    # see: https://github.com/huggingface/trl/pull/1255
    state = PartialState()
    with state.local_main_process_first():
        if script_args.dataset_name.endswith('.csv'):
            dataset = load_dataset('csv', data_files=f'{script_args.dataset_name}')
            dataset = prepare_dataset(dataset, script_args.dataset_name, script_args.dataset_train_split, templates=None)
            print("âœ… load dataset from csv successfully") # ğŸ” è°ƒè¯•
        else:
            dataset = load_dataset(script_args.dataset_name, name=script_args.dataset_config)

    ################
    # Training
    ################
    # â™»ï¸ è®­ç»ƒå‰æ¸…ç†å†…å­˜
    gc.collect()
    torch.cuda.empty_cache()
    
    trainer = SFTTrainer(
        model=model,
        args=training_args,
        data_collator=collate_fn,
        train_dataset=dataset[script_args.dataset_train_split],
        eval_dataset=dataset[script_args.dataset_test_split] if training_args.eval_strategy != "no" else None,
        processing_class=processor.tokenizer,
        peft_config=get_peft_config(model_args),
    )

    trainer.train()

    # Save and push to hub
    trainer.save_model(training_args.output_dir)
    if training_args.push_to_hub:
        trainer.push_to_hub(dataset_name=script_args.dataset_name)
        if trainer.accelerator.is_main_process:
            processor.push_to_hub(training_args.hub_model_id)
