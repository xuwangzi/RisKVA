#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®é›†éªŒè¯è„šæœ¬
æ£€æŸ¥ç”Ÿæˆçš„æ•°æ®é›†æ ¼å¼æ˜¯å¦ç¬¦åˆHugging Faceæ ‡å‡†
"""

import pandas as pd  # type: ignore
import os
import json
from PIL import Image
import argparse
from pathlib import Path

def get_script_directory():
    """è·å–è„šæœ¬æ‰€åœ¨ç›®å½•"""
    return os.path.dirname(os.path.abspath(__file__))

def validate_directory_structure(dataset_dir):
    """éªŒè¯æ•°æ®é›†ç›®å½•ç»“æ„"""
    print("ğŸ” æ£€æŸ¥ç›®å½•ç»“æ„...")
    
    # required_files = ['metadata.csv', 'dataset_info.json', 'README.md']
    required_files = ['metadata.csv', 'dataset_info.json']
    required_dirs = ['images']
    
    issues = []
    
    # æ£€æŸ¥å¿…éœ€æ–‡ä»¶
    for file in required_files:
        file_path = os.path.join(dataset_dir, file)
        if os.path.exists(file_path):
            print(f"  âœ… {file} å­˜åœ¨ ({os.path.getsize(file_path)} bytes)")
        else:
            issues.append(f"âŒ ç¼ºå°‘æ–‡ä»¶: {file}")
    
    # æ£€æŸ¥å¿…éœ€ç›®å½•
    for dir_name in required_dirs:
        dir_path = os.path.join(dataset_dir, dir_name)
        if os.path.exists(dir_path) and os.path.isdir(dir_path):
            image_count = len([f for f in os.listdir(dir_path) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp'))])
            print(f"  âœ… {dir_name}/ ç›®å½•å­˜åœ¨ ({image_count} å¼ å›¾ç‰‡)")
        else:
            issues.append(f"âŒ ç¼ºå°‘ç›®å½•: {dir_name}")
    
    return issues


def validate_metadata_csv(dataset_dir):
    """éªŒè¯metadata.csvæ–‡ä»¶"""
    print("\nğŸ“Š æ£€æŸ¥metadata.csv...")
    
    metadata_path = os.path.join(dataset_dir, 'metadata.csv')
    issues = []
    
    try:
        df = pd.read_csv(metadata_path)
        print(f"  âœ… æˆåŠŸè¯»å–CSVæ–‡ä»¶ï¼ŒåŒ…å« {len(df)} è¡Œæ•°æ®")
        
        # æ£€æŸ¥å¿…éœ€åˆ—
        required_columns = ['image_id', 'image_path', 'original_filename']
        for col in required_columns:
            if col in df.columns:
                print(f"  âœ… åŒ…å«å¿…éœ€åˆ—: {col}")
            else:
                issues.append(f"âŒ ç¼ºå°‘å¿…éœ€åˆ—: {col}")
        
        # æ£€æŸ¥æ•°æ®ç±»å‹å’Œå®Œæ•´æ€§
        if 'image_id' in df.columns:
            if df['image_id'].dtype in ['int64', 'int32']:
                print(f"  âœ… image_id æ•°æ®ç±»å‹æ­£ç¡®")
            else:
                issues.append(f"âŒ image_id æ•°æ®ç±»å‹é”™è¯¯: {df['image_id'].dtype}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç©ºå€¼
        null_counts = df.isnull().sum()
        if null_counts.sum() == 0:
            print(f"  âœ… æ— ç©ºå€¼")
        else:
            print(f"  âš ï¸  å­˜åœ¨ç©ºå€¼: {null_counts[null_counts > 0].to_dict()}")
        
        # æ˜¾ç¤ºåˆ—ä¿¡æ¯
        print(f"  ğŸ“‹ åŒ…å«åˆ—æ•°: {len(df.columns)}")
        print(f"  ğŸ“‹ åˆ—å: {list(df.columns)}")
        
        return df, issues
        
    except Exception as e:
        issues.append(f"âŒ è¯»å–metadata.csvå¤±è´¥: {e}")
        return None, issues


def validate_images(dataset_dir, df):
    """éªŒè¯å›¾ç‰‡æ–‡ä»¶"""
    print("\nğŸ–¼ï¸  æ£€æŸ¥å›¾ç‰‡æ–‡ä»¶...")
    
    images_dir = os.path.join(dataset_dir, 'images')
    issues = []
    
    if df is None:
        issues.append("âŒ æ— æ³•éªŒè¯å›¾ç‰‡ï¼šmetadata.csv è¯»å–å¤±è´¥")
        return issues
    
    # æ£€æŸ¥æ¯å¼ å›¾ç‰‡
    valid_images = 0
    image_sizes = []
    
    for idx, row in df.iterrows():
        image_path = os.path.join(dataset_dir, row['image_path'])
        
        if os.path.exists(image_path):
            try:
                with Image.open(image_path) as img:
                    image_sizes.append(img.size)
                    valid_images += 1
                    if idx < 3:  # åªæ˜¾ç¤ºå‰3å¼ çš„è¯¦ç»†ä¿¡æ¯
                        print(f"  âœ… {row['original_filename']}: {img.size} {img.format}")
            except Exception as e:
                issues.append(f"âŒ å›¾ç‰‡æŸå: {row['image_path']} - {e}")
        else:
            issues.append(f"âŒ å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {row['image_path']}")
    
    print(f"  ğŸ“Š æœ‰æ•ˆå›¾ç‰‡: {valid_images}/{len(df)}")
    
    if image_sizes:
        unique_sizes = list(set(image_sizes))
        print(f"  ğŸ“ å›¾ç‰‡å°ºå¯¸: {len(unique_sizes)} ç§ä¸åŒå°ºå¯¸")
        print(f"  ğŸ“ å°ºå¯¸èŒƒå›´: {min(image_sizes)} åˆ° {max(image_sizes)}")
    
    return issues


def validate_dataset_info(dataset_dir):
    """éªŒè¯dataset_info.json"""
    print("\nğŸ“„ æ£€æŸ¥dataset_info.json...")
    
    info_path = os.path.join(dataset_dir, 'dataset_info.json')
    issues = []
    
    try:
        with open(info_path, 'r', encoding='utf-8') as f:
            info = json.load(f)
        
        print(f"  âœ… æˆåŠŸè¯»å–JSONæ–‡ä»¶")
        
        # æ£€æŸ¥å¿…éœ€å­—æ®µ
        required_fields = ['dataset_name', 'total_samples', 'data_format']
        for field in required_fields:
            if field in info:
                print(f"  âœ… åŒ…å«å­—æ®µ: {field} = {info[field]}")
            else:
                issues.append(f"âŒ ç¼ºå°‘å­—æ®µ: {field}")
        
        return info, issues
        
    except Exception as e:
        issues.append(f"âŒ è¯»å–dataset_info.jsonå¤±è´¥: {e}")
        return None, issues


def test_huggingface_compatibility(dataset_dir):
    """æµ‹è¯•Hugging Faceå…¼å®¹æ€§"""
    print("\nğŸ¤— æµ‹è¯•Hugging Faceå…¼å®¹æ€§...")
    
    try:
        # æµ‹è¯•pandasè¯»å–
        df = pd.read_csv(os.path.join(dataset_dir, 'metadata.csv'))
        print(f"  âœ… Pandaså…¼å®¹: å¯ä»¥è¯»å– {len(df)} è¡Œæ•°æ®")
        
        # æµ‹è¯•å›¾ç‰‡è·¯å¾„
        first_image = os.path.join(dataset_dir, df.iloc[0]['image_path'])
        if os.path.exists(first_image):
            with Image.open(first_image) as img:
                print(f"  âœ… å›¾ç‰‡å¯è®¿é—®: {img.size} {img.format}")
        
        # æ¨¡æ‹Ÿdatasetsåº“çš„ä½¿ç”¨
        print("  ğŸ”§ æ¨¡æ‹ŸHugging Face datasetsä½¿ç”¨:")
        print("     from datasets import Dataset, Image as HFImage")
        print("     dataset = Dataset.from_pandas(df)")
        print("     dataset = dataset.cast_column('image_path', HFImage())")
        
        return True
        
    except Exception as e:
        print(f"  âŒ å…¼å®¹æ€§æµ‹è¯•å¤±è´¥: {e}")
        return False


def main():
    parser = argparse.ArgumentParser(description="éªŒè¯æ•°æ®é›†æ ¼å¼")
    parser.add_argument("dataset_dir", nargs='?', 
                       default=get_script_directory() + "/dataset",
                       help="æ•°æ®é›†ç›®å½•è·¯å¾„")
    
    args = parser.parse_args()
    dataset_dir = os.path.abspath(args.dataset_dir)
    
    print("=" * 80)
    print("ğŸ” æ•°æ®é›†éªŒè¯å·¥å…·")
    print("=" * 80)
    print(f"æ•°æ®é›†è·¯å¾„: {dataset_dir}")
    print()
    
    if not os.path.exists(dataset_dir):
        print(f"âŒ æ•°æ®é›†ç›®å½•ä¸å­˜åœ¨: {dataset_dir}")
        return
    
    all_issues = []
    
    # 1. éªŒè¯ç›®å½•ç»“æ„
    issues = validate_directory_structure(dataset_dir)
    all_issues.extend(issues)
    
    # 2. éªŒè¯metadata.csv
    df, issues = validate_metadata_csv(dataset_dir)
    all_issues.extend(issues)
    
    # 3. éªŒè¯å›¾ç‰‡æ–‡ä»¶
    issues = validate_images(dataset_dir, df)
    all_issues.extend(issues)
    
    # 4. éªŒè¯dataset_info.json
    info, issues = validate_dataset_info(dataset_dir)
    all_issues.extend(issues)
    
    # 5. æµ‹è¯•Hugging Faceå…¼å®¹æ€§
    test_huggingface_compatibility(dataset_dir)
    
    # æ€»ç»“
    print("\n" + "=" * 80)
    if all_issues:
        print("âŒ éªŒè¯å®Œæˆï¼Œå‘ç°ä»¥ä¸‹é—®é¢˜:")
        for issue in all_issues:
            print(f"  {issue}")
    else:
        print("âœ… éªŒè¯å®Œæˆï¼Œæ•°æ®é›†æ ¼å¼å®Œå…¨æ­£ç¡®ï¼")
        print("\nğŸ¯ æ•°æ®é›†å¯ä»¥ç›´æ¥ç”¨äº:")
        print("  â€¢ Hugging Face datasetsåº“")
        print("  â€¢ PyTorch DataLoader")
        print("  â€¢ TensorFlow tf.data")
        print("  â€¢ è‡ªå®šä¹‰è®­ç»ƒè„šæœ¬")
    
    print("=" * 80)


if __name__ == "__main__":
    main() 